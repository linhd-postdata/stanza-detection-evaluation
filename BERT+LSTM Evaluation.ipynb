{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModel, BertTokenizerFast, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download model\n",
    "!wget https://storage.googleapis.com/postdata-models/stanzas/eval/saved_weights_bert.pt -O bert_data/saved_weights_bert.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_NAME = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_lstm(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT_lstm, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(BERT_NAME)\n",
    "        self.lstm = nn.LSTM(768, 768, batch_first=True, num_layers=3, dropout=0.2, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(768*2, 300)\n",
    "        self.linear2 = nn.Linear(300, 46)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        self.bert.config.return_dict=False\n",
    "        self.bert.config.output_hidden_states=True\n",
    "        with torch.no_grad():  # disable gradient calculation to freeze the model\n",
    "            # pass the inputs to the model\n",
    "            cls_emb, pool, hidden_states, *_ = self.bert(sent_id, attention_mask=mask)\n",
    "        output, (h_n, c_n) = self.lstm(cls_emb)\n",
    "        out = self.linear1(output)\n",
    "        out = torch.sum(out, 1).squeeze(1)\n",
    "        out = nn.functional.gelu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_seq = pickle.load(open(\"bert_data/test_seq.p\",\"rb\"))\n",
    "test_mask = pickle.load(open(\"bert_data/test_mask.p\",\"rb\"))\n",
    "test_y = pickle.load(open(\"bert_data/test_y.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_lstm()\n",
    "path = 'bert_data/saved_weights_bert.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq, test_mask)\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = np.argmax(preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6000    0.1034    0.1765        29\n",
      "           1     0.5000    0.1818    0.2667        11\n",
      "           2     0.7500    0.6429    0.6923        14\n",
      "           3     0.4737    0.6429    0.5455        14\n",
      "           4     0.2708    0.4643    0.3421        28\n",
      "           5     0.0000    0.0000    0.0000         2\n",
      "           6     0.5000    0.1429    0.2222        28\n",
      "           7     0.4667    0.2500    0.3256        28\n",
      "           8     0.4444    0.8889    0.5926        27\n",
      "           9     0.3636    0.1538    0.2162        26\n",
      "          10     0.3333    0.0769    0.1250        26\n",
      "          11     0.1237    1.0000    0.2202        12\n",
      "          12     0.3478    0.5333    0.4211        15\n",
      "          13     0.5000    0.7857    0.6111        14\n",
      "          14     0.2712    0.5926    0.3721        27\n",
      "          15     0.2727    0.5000    0.3529        12\n",
      "          16     0.6000    0.2143    0.3158        28\n",
      "          17     0.6471    0.3929    0.4889        28\n",
      "          18     0.6429    0.7500    0.6923        24\n",
      "          19     0.4815    0.4643    0.4727        28\n",
      "          20     0.8333    0.1724    0.2857        29\n",
      "          21     0.5789    0.4074    0.4783        27\n",
      "          22     0.6250    0.5357    0.5769        28\n",
      "          23     0.7895    0.5172    0.6250        29\n",
      "          24     0.7500    0.6000    0.6667        25\n",
      "          25     0.2727    0.3333    0.3000        18\n",
      "          26     0.6667    0.5714    0.6154        28\n",
      "          27     0.2188    0.4828    0.3011        29\n",
      "          28     1.0000    0.5385    0.7000        26\n",
      "          29     0.7073    1.0000    0.8286        29\n",
      "          30     0.6667    0.1538    0.2500        26\n",
      "          31     0.5000    0.4074    0.4490        27\n",
      "          32     0.1000    0.0526    0.0690        19\n",
      "          33     0.1000    0.4000    0.1600         5\n",
      "          34     0.3333    0.1429    0.2000         7\n",
      "          35     0.2000    0.0645    0.0976        31\n",
      "          36     1.0000    0.2222    0.3636         9\n",
      "          37     0.3158    0.2857    0.3000        21\n",
      "          38     0.4286    0.3750    0.4000         8\n",
      "          39     1.0000    0.2667    0.4211        30\n",
      "          40     0.3200    0.2500    0.2807        32\n",
      "          41     0.5079    1.0000    0.6737        32\n",
      "          42     0.2727    0.6000    0.3750        10\n",
      "          43     0.5161    0.4706    0.4923        34\n",
      "          44     0.2857    0.4000    0.3333         5\n",
      "          45     1.0000    0.3750    0.5455        16\n",
      "\n",
      "    accuracy                         0.4276      1001\n",
      "   macro avg     0.4908    0.4219    0.3965      1001\n",
      "weighted avg     0.5228    0.4276    0.4166      1001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, preds, zero_division=0, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
