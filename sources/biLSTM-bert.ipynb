{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast, BertTokenizer\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AdamW\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "from pylab import rcParams\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data_path = 'data/evaluation-final.csv'\n",
    "#raw_data_path = 'annotation-final-sorted-filtered.csv'\n",
    "raw_data_path = 'annotation-final-baseline-rev.csv'\n",
    "\n",
    "df = pd.read_csv(raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    output = string.strip()\n",
    "    # replacements = ((\"“\", '\"'), (\"”\", '\"'), (\"//\", \"\"), (\"«\", '\"'), (\"»\",'\"'))\n",
    "    # Quitamos símbolos que no aportan información\n",
    "    replacements = (\n",
    "        (\"_\", \" \"), (\"x000D\", \" \"), (\"-\", \" \"),\n",
    "        (\"[\", \"\"), (\"]\", \"\"), (\"'\", \"\"), (\"#\", \" \"), (\"*\", \"\")\n",
    "    )\n",
    "    for replacement in replacements:\n",
    "        output = output.replace(*replacement)\n",
    "    # Any sequence of two or more spaces should be converted into one space\n",
    "    #output = re.sub(r'(?is)\\s+', ' ', output)\n",
    "    output = re.sub(r'[0-9]', '', output)  # Quitar números\n",
    "    output = re.sub(r'\\t+', ' ', output)  # Cambiar tabulaciones por espacios\n",
    "    output = re.sub(r'\\n{2,}', '\\n', output)  # Cambiar varios saltos de línea seguidos por uno solo\n",
    "    output = re.sub(r' {2,}', ' ', output)  # Cambiar varios espacios seguidos por uno solo\n",
    "\n",
    "    return output.strip()\n",
    "\n",
    "def clean_labels(label):\n",
    "    return \"unknown\" if str(label) == \"None\" else label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    output = string.strip()\n",
    "    # replacements = ((\"“\", '\"'), (\"”\", '\"'), (\"//\", \"\"), (\"«\", '\"'), (\"»\",'\"'))\n",
    "    replacements = (\n",
    "      (\"“\", ''), (\"”\", ''), (\"//\", \"\"), (\"«\", ''), (\"»\", ''), (\",\", ''),\n",
    "      (\";\", ''), (\".\", ''),\n",
    "        # (\"?\", ''), (\"¿\", ''), (\"¡\", ''), (\"!\", ''), (\"-\", ' '),\n",
    "    )\n",
    "    for replacement in replacements:\n",
    "        output = output.replace(*replacement)\n",
    "    # Any sequence of two or more spaces should be converted into one space\n",
    "    \n",
    "    #output = re.sub(r'\\t+', ' ', output)\n",
    "    #output = re.sub(r'\\n{2,}', '', output)\n",
    "    #output = re.sub(r'^\\s+\\*.*\\n', '', output)\n",
    "    \n",
    "    output = re.sub(r'[0-9]', '', output)  # Quitar números\n",
    "    output = re.sub(r'\\t+', ' ', output)  # Cambiar tabulaciones por espacios\n",
    "    output = re.sub(r'\\n{2,}', '\\n', output)  # Cambiar varios saltos de línea seguidos por uno solo\n",
    "    #output = re.sub(r'\\n', ' <CR> ', output)  # Cambiar saltos de línea por etiqueta <CR>\n",
    "    output = re.sub(r' {2,}', ' ', output)  # Cambiar varios espacios seguidos por uno solo\n",
    "    return output.strip()\n",
    "\n",
    "\n",
    "def clean_labels(label):\n",
    "    return \"unknown\" if str(label) == \"None\" else label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stanza_text</th>\n",
       "      <th>ST_Correct</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Year</th>\n",
       "      <th>Copyrighted</th>\n",
       "      <th>Tipo de rima</th>\n",
       "      <th>Información métrica</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La catedral de Barcelona dice: \\nSe levantan, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>?</td>\n",
       "      <td>Miguel de Unamuno</td>\n",
       "      <td>1936</td>\n",
       "      <td>no</td>\n",
       "      <td>Sin rima</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No estás ya aquí. Lo que veo\\nde ti, cuerpo, e...</td>\n",
       "      <td>None</td>\n",
       "      <td>?</td>\n",
       "      <td>Pedro Salinas</td>\n",
       "      <td>1951</td>\n",
       "      <td>si</td>\n",
       "      <td>Sin rima</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perdóname, Fray luan, si profano tu lira\\nun t...</td>\n",
       "      <td>None</td>\n",
       "      <td>Lámparas de fuego</td>\n",
       "      <td>Gerardo de Diego</td>\n",
       "      <td>1987</td>\n",
       "      <td>si</td>\n",
       "      <td>Sin rima</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Es el sol un bello lucero,\\nque cada mañana me...</td>\n",
       "      <td>None</td>\n",
       "      <td>De su libro: Amor en un suspiro</td>\n",
       "      <td>Vicente Devesa Llobel</td>\n",
       "      <td>?</td>\n",
       "      <td>si</td>\n",
       "      <td>Sin rima</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Como me gustaría poder ser voz,\\ncomo me gusta...</td>\n",
       "      <td>None</td>\n",
       "      <td>De su libro: Amor en un suspiro</td>\n",
       "      <td>Vicente Devesa Llobel</td>\n",
       "      <td>?</td>\n",
       "      <td>si</td>\n",
       "      <td>Sin rima</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>Verdugos de ideales afligieron la tierra,\\nen ...</td>\n",
       "      <td>terceto_monorrimo</td>\n",
       "      <td>Canto de esperanza</td>\n",
       "      <td>Rubén Darío</td>\n",
       "      <td>1916</td>\n",
       "      <td>No</td>\n",
       "      <td>Consonante</td>\n",
       "      <td>14A 14A 14A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>¡Oh, Señor Jesucristo! ¡Por qué tardas, qué es...</td>\n",
       "      <td>terceto_monorrimo</td>\n",
       "      <td>Canto de esperanza</td>\n",
       "      <td>Rubén Darío</td>\n",
       "      <td>1916</td>\n",
       "      <td>No</td>\n",
       "      <td>Consonante</td>\n",
       "      <td>14A 14A 13A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002</th>\n",
       "      <td>Surge de pronto y vierte la esencia de la vida...</td>\n",
       "      <td>terceto_monorrimo</td>\n",
       "      <td>Canto de esperanza</td>\n",
       "      <td>Rubén Darío</td>\n",
       "      <td>1916</td>\n",
       "      <td>No</td>\n",
       "      <td>Consonante</td>\n",
       "      <td>14A 14A 14A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5003</th>\n",
       "      <td>Ven, Señor, para hacer la gloria de Ti mismo;\\...</td>\n",
       "      <td>terceto_monorrimo</td>\n",
       "      <td>Canto de esperanza</td>\n",
       "      <td>Rubén Darío</td>\n",
       "      <td>1916</td>\n",
       "      <td>No</td>\n",
       "      <td>Consonante</td>\n",
       "      <td>13A 14A 13A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>Y tu caballo blanco, que miró el visionario,\\n...</td>\n",
       "      <td>terceto_monorrimo</td>\n",
       "      <td>Canto de esperanza</td>\n",
       "      <td>Rubén Darío</td>\n",
       "      <td>1916</td>\n",
       "      <td>No</td>\n",
       "      <td>Consonante</td>\n",
       "      <td>14A 14A 13A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5005 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Stanza_text         ST_Correct  \\\n",
       "0     La catedral de Barcelona dice: \\nSe levantan, ...               None   \n",
       "1     No estás ya aquí. Lo que veo\\nde ti, cuerpo, e...               None   \n",
       "2     Perdóname, Fray luan, si profano tu lira\\nun t...               None   \n",
       "3     Es el sol un bello lucero,\\nque cada mañana me...               None   \n",
       "4     Como me gustaría poder ser voz,\\ncomo me gusta...               None   \n",
       "...                                                 ...                ...   \n",
       "5000  Verdugos de ideales afligieron la tierra,\\nen ...  terceto_monorrimo   \n",
       "5001  ¡Oh, Señor Jesucristo! ¡Por qué tardas, qué es...  terceto_monorrimo   \n",
       "5002  Surge de pronto y vierte la esencia de la vida...  terceto_monorrimo   \n",
       "5003  Ven, Señor, para hacer la gloria de Ti mismo;\\...  terceto_monorrimo   \n",
       "5004  Y tu caballo blanco, que miró el visionario,\\n...  terceto_monorrimo   \n",
       "\n",
       "                                Title                 Author  Year  \\\n",
       "0                                   ?      Miguel de Unamuno  1936   \n",
       "1                                   ?          Pedro Salinas  1951   \n",
       "2                   Lámparas de fuego       Gerardo de Diego  1987   \n",
       "3     De su libro: Amor en un suspiro  Vicente Devesa Llobel     ?   \n",
       "4     De su libro: Amor en un suspiro  Vicente Devesa Llobel     ?   \n",
       "...                               ...                    ...   ...   \n",
       "5000               Canto de esperanza            Rubén Darío  1916   \n",
       "5001               Canto de esperanza            Rubén Darío  1916   \n",
       "5002               Canto de esperanza            Rubén Darío  1916   \n",
       "5003               Canto de esperanza            Rubén Darío  1916   \n",
       "5004               Canto de esperanza            Rubén Darío  1916   \n",
       "\n",
       "     Copyrighted Tipo de rima Información métrica Unnamed: 8  \n",
       "0             no     Sin rima                 NaN        NaN  \n",
       "1             si     Sin rima                 NaN        NaN  \n",
       "2             si     Sin rima                 NaN        NaN  \n",
       "3             si     Sin rima                 NaN        NaN  \n",
       "4             si     Sin rima                 NaN        NaN  \n",
       "...          ...          ...                 ...        ...  \n",
       "5000          No   Consonante         14A 14A 14A        NaN  \n",
       "5001          No   Consonante         14A 14A 13A        NaN  \n",
       "5002          No   Consonante         14A 14A 14A        NaN  \n",
       "5003          No   Consonante         13A 14A 13A        NaN  \n",
       "5004          No   Consonante         14A 14A 13A        NaN  \n",
       "\n",
       "[5005 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"n_verses\"] = df[\"Stanza_text\"].apply(lambda x: x.count('\\n')+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stanza_text</th>\n",
       "      <th>ST_Correct</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Year</th>\n",
       "      <th>Copyrighted</th>\n",
       "      <th>Tipo de rima</th>\n",
       "      <th>Información métrica</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>n_verses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Stanza_text, ST_Correct, Title, Author, Year, Copyrighted, Tipo de rima, Información métrica, Unnamed: 8, n_verses]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['n_verses'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_verses'] = df['n_verses'].apply(lambda x: 5 if x==1 else x)\n",
    "df['n_verses'] = df['n_verses'].apply(lambda x: 2 if x==1 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    df_out = pd.DataFrame()\n",
    "    df = (pd\n",
    "        .read_csv(raw_data_path)\n",
    "        .rename(columns={\"Stanza_text\": \"text\", \"ST_Correct\": \"stanza\"})\n",
    "        .assign(\n",
    "            text=lambda x: x[\"text\"].apply(clean_text),\n",
    "            stanza=lambda x: x[\"stanza\"].apply(clean_labels),\n",
    "        )\n",
    "    )\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_out[\"text\"] = df[\"text\"]\n",
    "    df_out[\"label\"] = label_encoder.fit_transform(df[\"stanza\"])\n",
    "    return df_out, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full, label_encoder = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[\"n_verses\"] = df[\"n_verses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"n_verses\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "#train_df, test_df = train_test_split(df_full,stratify=df_full[\"label\"], test_size=0.2, random_state=4242)\n",
    "#train_df, test_df = train_test_split(df_full,stratify=df_full[\"n_verses\"], test_size=0.2, random_state=4242)\n",
    "X_test = list(test_df['text'])\n",
    "y_test = list(test_df['label'])\n",
    "#y_test = list(test_df['n_verses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv(r'train_df.csv', index = False)\n",
    "#test_df.to_csv(r'test_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(train_df['text'])\n",
    "y = list(train_df['label'])\n",
    "#y = list(train_df['n_verses'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,stratify=train_df[\"label\"], test_size=0.2, random_state=4242)\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(X, y,stratify=train_df[\"label\"], test_size=0.2, random_state=4242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT_NAME = 'bert-base-multilingual-cased'\n",
    "BERT_NAME = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "#BERT_NAME = 'mrm8488/bert-spanish-cased-finetuned-pos'\n",
    "#BERT_NAME = 'xlm-roberta-large-finetuned-conll02-spanish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "#tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x403.2 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in X_train]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_EVAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 300\n",
    "STRIDE_LENGTH = 10\n",
    "# tokenize and encode sequences in the training set\n",
    "if USE_EVAL:\n",
    "    # tokenize and encode sequences in the validation set\n",
    "    tokens_val = tokenizer.batch_encode_plus(\n",
    "        X_valid,\n",
    "        padding=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=STRIDE_LENGTH\n",
    "    )\n",
    "    tokens_train = tokenizer.batch_encode_plus(\n",
    "    X_train,\n",
    "    padding=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=STRIDE_LENGTH\n",
    "    )\n",
    "else: \n",
    "    tokens_train = tokenizer.batch_encode_plus(\n",
    "    X,\n",
    "    padding=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=STRIDE_LENGTH\n",
    "    )\n",
    "    \n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    X_test,\n",
    "    padding=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=STRIDE_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_EVAL:\n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "    train_y = torch.tensor(y_train)\n",
    "    train_y = nn.functional.pad(input=train_y, pad=(0, train_mask.size()[0]-train_y.size()[0]), mode='constant', value=0)\n",
    "\n",
    "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "    val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "    val_y = torch.tensor(y_valid)\n",
    "    val_y = nn.functional.pad(input=val_y, pad=(0, val_mask.size()[0]-val_y.size()[0]), mode='constant', value=0)\n",
    "else:\n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "    train_y = torch.tensor(y)\n",
    "    train_y = nn.functional.pad(input=torch.tensor(train_y), pad=(0, train_mask.size()[0]-train_y.size()[0]), mode='constant', value=0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(y_test)\n",
    "test_y = nn.functional.pad(input=test_y, pad=(0, test_mask.size()[0]-test_y.size()[0]), mode='constant', value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_y, open(\"test_y.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a batch size\n",
    "batch_size = 16 #150\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "if USE_EVAL:\n",
    "    # wrap tensors\n",
    "    val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "    # sampler for sampling the data during training\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    # dataLoader for validation set\n",
    "    val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step,\n",
    "                                                       len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()\n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    # returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            #elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print(\n",
    "                '  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_lstm(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT_lstm, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(BERT_NAME)\n",
    "        self.lstm = nn.LSTM(768, 768, batch_first=True, num_layers=3, dropout=0.2, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(768*2, 300)\n",
    "        self.linear2 = nn.Linear(300, 46)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        self.bert.config.return_dict=False\n",
    "        self.bert.config.output_hidden_states=True\n",
    "        with torch.no_grad():  # disable gradient calculation to freeze the model\n",
    "            # pass the inputs to the model\n",
    "            cls_emb, pool, hidden_states, *_ = self.bert(sent_id, attention_mask=mask)\n",
    "        output, (h_n, c_n) = self.lstm(cls_emb)\n",
    "        out = self.linear1(output)\n",
    "        out = torch.sum(out, 1).squeeze(1)\n",
    "        out = nn.functional.gelu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_lstm()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.bert.config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "#optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "# define the loss function\n",
    "#cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "cross_entropy  = nn.CrossEntropyLoss(weight=weights) \n",
    "# number of training epochs\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial loss = infinite\n",
    "best_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    train_loss, _ = train()\n",
    "    if USE_EVAL:\n",
    "        valid_loss, _ = evaluate()\n",
    "        # save best model\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    else:\n",
    "        # save best model\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "        \n",
    "    train_losses.append(train_loss)\n",
    "    if USE_EVAL:\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    if USE_EVAL:\n",
    "        print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights_bert.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_lstm()\n",
    "path = 'saved_weights_bert.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq, test_mask)\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(preds, open(\"bert_preds.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_seq, open(\"test_seq.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_mask, open(\"test_mask.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3333    0.0690    0.1143        29\n",
      "           1     0.5000    0.1818    0.2667        11\n",
      "           2     0.7500    0.6429    0.6923        14\n",
      "           3     0.4500    0.6429    0.5294        14\n",
      "           4     0.2766    0.4643    0.3467        28\n",
      "           5     0.0000    0.0000    0.0000         2\n",
      "           6     0.5000    0.1429    0.2222        28\n",
      "           7     0.4667    0.2500    0.3256        28\n",
      "           8     0.4444    0.8889    0.5926        27\n",
      "           9     0.4167    0.1923    0.2632        26\n",
      "          10     0.3333    0.0769    0.1250        26\n",
      "          11     0.1277    1.0000    0.2264        12\n",
      "          12     0.3478    0.5333    0.4211        15\n",
      "          13     0.5000    0.7857    0.6111        14\n",
      "          14     0.2833    0.6296    0.3908        27\n",
      "          15     0.2727    0.5000    0.3529        12\n",
      "          16     0.6364    0.2500    0.3590        28\n",
      "          17     0.6471    0.3929    0.4889        28\n",
      "          18     0.6429    0.7500    0.6923        24\n",
      "          19     0.4643    0.4643    0.4643        28\n",
      "          20     0.8333    0.1724    0.2857        29\n",
      "          21     0.5789    0.4074    0.4783        27\n",
      "          22     0.6250    0.5357    0.5769        28\n",
      "          23     0.7895    0.5172    0.6250        29\n",
      "          24     0.7500    0.6000    0.6667        25\n",
      "          25     0.2727    0.3333    0.3000        18\n",
      "          26     0.6667    0.5714    0.6154        28\n",
      "          27     0.2154    0.4828    0.2979        29\n",
      "          28     1.0000    0.5385    0.7000        26\n",
      "          29     0.7073    1.0000    0.8286        29\n",
      "          30     0.6667    0.1538    0.2500        26\n",
      "          31     0.5238    0.4074    0.4583        27\n",
      "          32     0.1000    0.0526    0.0690        19\n",
      "          33     0.1000    0.4000    0.1600         5\n",
      "          34     0.5000    0.1429    0.2222         7\n",
      "          35     0.2000    0.0645    0.0976        31\n",
      "          36     1.0000    0.2222    0.3636         9\n",
      "          37     0.3333    0.2857    0.3077        21\n",
      "          38     0.4286    0.3750    0.4000         8\n",
      "          39     1.0000    0.2667    0.4211        30\n",
      "          40     0.2963    0.2500    0.2712        32\n",
      "          41     0.5079    1.0000    0.6737        32\n",
      "          42     0.2857    0.6000    0.3871        10\n",
      "          43     0.5333    0.4706    0.5000        34\n",
      "          44     0.2857    0.4000    0.3333         5\n",
      "          45     1.0000    0.3750    0.5455        16\n",
      "\n",
      "    accuracy                         0.4296      1001\n",
      "   macro avg     0.4912    0.4235    0.3982      1001\n",
      "weighted avg     0.5192    0.4296    0.4181      1001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds, zero_division=0, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y, preds, zero_division=0, digits = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20, 10\n",
    "plt.plot(range(1,len(train_losses)+1), train_losses, label='Train')\n",
    "if USE_EVAL:\n",
    "    plt.plot(range(1,len(valid_losses)+1), valid_losses, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "leg = plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.inverse_transform([12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
